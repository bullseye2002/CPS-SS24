{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e33b0acc",
   "metadata": {},
   "source": [
    "<img src=\"res/th-logo.png\" width=\"100\" align=\"left\"/>\n",
    "<img src=\"res/th-bar.png\" width=\"800\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b45e05",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"> WPF Cyber-physische Systeme (CPS) </h1><br>\n",
    "<h2 align=\"center\"> Niryo NED2 </h2><br><br>\n",
    "\n",
    "<img src=\"res/Niryo_logo.png\" width=\"300\"/><br><br>\n",
    "<img src=\"res/titelbild.png\" width=\"500\"/><br><br><br>\n",
    "\n",
    "\n",
    "<h4 align=\"center\"> System </center><br>\n",
    "<img src=\"res/ros_logo.png\" width=\"100\"/><br><br>\n",
    "\n",
    "Der Roboterarm NED2 läuft unter der Middleware **ROS**.\n",
    "\n",
    "\n",
    "\n",
    "<h4 align=\"center\"> NiryoStudio </h4><br>\n",
    "Kontrolliert werden kann der NED2 über das NiryoStudio. Über dieses Programm muss der Roboter immer mit dem Rechner verbunden werden, damit der Code auf dem Roboter ausgeführt werden kann. In Sciebo liegt die Anwendung im Pfad: WPF_CPS_SoSe23\\NED2. Alternativ kann diese unter dem folgenden Link heruntergeladen werden (dazu müssen Sie sich allerdings bei Niryo registrieren und benötigen die Seriennr. unseres Roboterarms):\n",
    "https://docs.niryo.com/product/niryo-studio/v4.1.1/en/source/download_and_installation.html\n",
    "\n",
    "<h4 align=\"center\">  Programmierung </h4><br>\n",
    "<img src=\"res/PyNiryo_logo_2.png\" width=\"200\"/><br><br>\n",
    "\n",
    "Programmiert werden kann der NED2 in jeder Entwicklungsumgebung, die sich auf dem durch NiryoStudio verbundenen Rechner befindet. Der Code kann hier in den Notebooks ausgeführt werden.\n",
    "\n",
    "In diesem WPF wird Python mit dem Package **PyNiryo** genutzt, um mit dem NED2 zu interagieren. Im Folgenden werden Sie ein bereits vorbereitetes Virtual Environment aktivieren, wenn Sie dieses Notebook nicht aus einer Anaconda Umgebung heraus gestartet haben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47c8996",
   "metadata": {},
   "source": [
    "## Projekt 1: Fertigungsprozess mit Niryo NED2\n",
    "Die folgenden Aufgabenblöcke können unter den Teams mit jeweils einem Thema aufgeteilt werden. Es sollen verschiedene Aufgaben zur Implementierung eines industriellen Fertigungsprozesses gelöst werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fb5a35",
   "metadata": {},
   "source": [
    "### NED2 mit Vision-Funktion und Förderband: Transfer gefertigter Objekte zur Versandabteilung\n",
    "Sie sind für die Programmierung eines Roboter-Arms einer Fertigungsstraße zuständig. Gefertigte Objekte werden in ein Zwischenlager gebracht, bevor sie auf Fehler überprüft und zum Transport in die Versandabteilung auf ein Förderband gelegt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ea333",
   "metadata": {},
   "source": [
    "#### Workspace einrichten\n",
    "Die erste Aufgabe besteht darin, das Zwischenlager als Workspace in NiryoStudio einzurichten. Folgen Sie dazu den Instruktionen unter dem Reiter \"Vision Module\".\n",
    "Positionieren Sie das Feld mit den runden Markierungen vor dem NED2. Platzieren Sie mehrere Objekte in der Mitte des Felds. Erstellen Sie in NiryoStudio einen neuen Workspace Warenlager. Beachten Sie dabei die Instruktionen des Programms, insbesondere das Lösen des Greifers und das Platzieren des Kalibrierungsmoduls für Workspaces. Schalten Sie den NED2 vor den Montierungsarbeiten unbedingt aus, um Schäden durch ungewollte Bewegungen oder Kontakt mit unter elektrischer Spannung stehenden Anschlüssen zu vermeiden! \n",
    "\n",
    "<img src=\"res/niryostudio_workspace_zwischenlager.png\" width=\"600\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a33a6d7",
   "metadata": {},
   "source": [
    "#### Konfiguration der Arbeitsumgebung\n",
    "Dieser Schritt wird nur benötigt, wenn Sie nicht mit Anaconda arbeiten. \n",
    "\n",
    "Führen Sie nun den nachfolgenden Code aus, um Ihr System zur Interaktion mit dem NED2 zu konfigurieren. Das Venv wurde bereits erzeugt, normalerweise müssen die auskommentierten Anweisungen nicht mehr ausgeführt werden! Bei Ausführung des folgenden Blocks sollte das Venv \"niryo_venv\" als aktive Umgebung angezeigt werden, das alle nötigen Module enthält, die für die folgenden Aufgaben benötigt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8193d00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:28:05.417348Z",
     "start_time": "2024-06-14T11:28:05.399379Z"
    }
   },
   "outputs": [],
   "source": [
    "# Virtual Environment installieren (muss nur einmal gemacht werden)\n",
    "#! python3 -m venv niryo_venv\n",
    "#! pip3 install pyniryo\n",
    "\n",
    "# Virtual Environment einrichten\n",
    "#! source niryo_venv/bin/activate\n",
    "#! pip3 -V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b9fe96",
   "metadata": {},
   "source": [
    "#### Grundeinstellungen des NED2\n",
    "Der folgende Block legt die Grundeinstellungen des NED2 fest. Normalerweise muss hier nichts mehr verändert werden (mit simulation_mode bestimmen Sie, ob Sie mit der Simulation oder mit dem realen Roboter arbeiten. Im Simulationsmodus müssen Sie vermutlich auch die robot_ip_address anpassen). Führen Sie die folgenden beiden Blöcke aus, um mit dem NED2 interagieren zu können. Sie werden eine visuelle und akustische Rückmeldung vom NED2 erhalten (in der Simulation nicht)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1b13fb8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:28:07.211955Z",
     "start_time": "2024-06-14T11:28:05.421393Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://docs.niryo.com/dev/pyniryo2/v1.0.0/en/index.html\n",
    "from pyniryo2 import *\n",
    "\n",
    "# Sie werden auch für bestimmte Funktionen pyniryo brauchen. Zum Beispiel uncompress_image, undistort_image.\n",
    "# https://docs.niryo.com/dev/pyniryo/v1.1.2/en/index.html\n",
    "import pyniryo\n",
    "\n",
    "\n",
    "# Netzwerk\n",
    "hotspot_mode = \"10.10.10.10\"\n",
    "wifi_mode = \"192.168.0.140\"\n",
    "\n",
    "# -- MUST Change these variables\n",
    "simulation_mode = False\n",
    "if simulation_mode:\n",
    "    robot_ip_address, workspace_name = \"192.168.209.128\", \"gazebo_1\"\n",
    "else:\n",
    "    robot_ip_address, workspace_name = wifi_mode, \"cps_praktikum\"\n",
    "    \n",
    "\n",
    "# -- Can Change these variables\n",
    "grid_dimension = (3, 3)  # conditioning grid dimension\n",
    "vision_process_on_robot = False  # boolean to indicate if the image processing happens on the Robot\n",
    "display_stream = True  # Only used if vision on computer\n",
    "\n",
    "# -- Should Change these variables\n",
    "# The pose from where the image processing happens\n",
    "observation_pose = PoseObject(\n",
    "    x=0.17, y=0., z=0.35,\n",
    "    roll=0.0, pitch=1.57, yaw=0.0,\n",
    ")\n",
    "\n",
    "# Center of the conditioning area\n",
    "center_conditioning_pose = PoseObject(\n",
    "    x=0.0, y=-0.25, z=0.12,\n",
    "    roll=-0., pitch=1.57, yaw=-1.57\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e385931f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-14T11:28:13.004698Z",
     "start_time": "2024-06-14T11:28:07.214035Z"
    }
   },
   "outputs": [],
   "source": [
    "# Connect to robot\n",
    "robot_ip_address = '192.168.187.128'\n",
    "robot = NiryoRobot(robot_ip_address)\n",
    "\n",
    "if (not simulation_mode):\n",
    "    # Calibrate robot if robot needs calibration\n",
    "    robot.arm.calibrate_auto()\n",
    "    # Equip tool\n",
    "    robot.tool.update_tool()\n",
    "\n",
    "# Launching main process\n",
    "# Der Roboter braucht eine Verbindung zum Internet für robot.sound.say\n",
    "#ROBOT.SOUND.SAY FUNKTIONIERT NICHT: \"Message : Wrong robot hardware version, feature only available on Ned2\")\n",
    "#Annahme, funktioniert nicht in der Simulation\n",
    "#robot.sound.say(\"Configuration successful\", 0)\n",
    "# Ending\n",
    "robot.arm.go_to_sleep()\n",
    "# Releasing connection\n",
    "#robot.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7f6f83",
   "metadata": {},
   "source": [
    "#### Objekte greifen und positionieren\n",
    "Programmieren Sie eine Funktion, die Objekte aufgrund von visuellen Merkmalen (Farbe, Form) voneinander unterscheiden kann. Die gefertigten Objekttypen sind vorher bekannt. Es sollte eine entsprechende Rückmeldung von NED2 erfolgen, wenn ein Objekt erkannt wurde (z.B. Markierung im Bild).\n",
    "\n",
    "<img src=\"res/python_image_tresh_any.png\" width=\"600\" align=\"left\"/>\n",
    "\n",
    "Sobald die Objekte sicher erkannt werden können, können diese vom Zwischenlager auf das Förderband in Richtung Verpackungsprozess gelegt werden. Erweitern Sie die Funktion aus der letzten Aufgabe, sodass alle Objekte im Lager nacheinander gegriffen und über der Rampe losgelassen werden, damit diese auf das Förderband gelangen.\n",
    "\n",
    "Testen Sie Ihren Code zunächst in der Simulation bevor Sie diesen am realen Roboter testen. In der Simulation gibt es keine Rampe. In der Simulation können die Objekte deshalb an irgendeinem Ort abgelegt werden. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ebf738",
   "metadata": {},
   "source": [
    "#### Hauptprogramm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "727f58b7-dc9a-466f-8c3c-a217d7b088f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ObjectColor.BLUE\n",
      "ObjectColor.RED\n"
     ]
    }
   ],
   "source": [
    "#platziert alle Objekte an einen Ort\n",
    "offset_size = 0.05\n",
    "workspace_name = 'gazebo_1'\n",
    "# The pick pose\n",
    "pick_pose = PoseObject(\n",
    "    x=0.25, y=0., z=0.15,\n",
    "    roll=-0.0, pitch=1.57, yaw=0.0,\n",
    ")\n",
    "# The Place pose\n",
    "place_pose = PoseObject(\n",
    "    x=0.0, y=-0.25, z=0.1,\n",
    "    roll=0.0, pitch=1.57, yaw=-1.57)\n",
    "\n",
    "delete_pose= PoseObject(\n",
    "    x=0.225, y=0.32, z=0.08,\n",
    "    roll=-0.0, pitch=1.4, yaw=0.8)\n",
    "\n",
    "robot.arm.move_pose(observation_pose)\n",
    "\n",
    "#robot.arm.set_learning_mode(True)\n",
    "obj_found, shape_ret, color_ret = robot.vision.vision_pick(workspace_name)\n",
    "catch_count = 0\n",
    "while obj_found:\n",
    "    next_place_pose = place_pose.copy_with_offsets(x_offset= catch_count * offset_size)\n",
    "    robot.pick_place.place_from_pose(next_place_pose)\n",
    "    robot.tool.open_gripper()\n",
    "    robot.arm.move_pose(observation_pose)\n",
    "    obj_found, shape_ret, color_ret = robot.vision.vision_pick(workspace_name)\n",
    "    if obj_found == False:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cb47e9",
   "metadata": {},
   "source": [
    "#### Optional: Fehlerhafte Objekte aussortieren\n",
    "Leider ist der Produktionsabteilung ein Fehler unterlaufen: Es wurden keine roten und keine blauen, runden Objekte bestellt. Identifizieren Sie Objekte, die diese Eigenschaften erfüllen, um diese nachträglich vom Förderband zu entfernen. Dies muss allerdings spätestens bis zur zweiten Markierung geschehen, da die Objekte sonst in die Versandabteilung gelangen. NED2 sollte beide Aufgaben abwechselnd erledigen. In der Praxis könnten hier zwei verschiedene Roboter-Arme zum Einsatz kommen. NED2 könnte hier bspw. drei Objekte aus dem Lager greifen und platzieren, bevor er an anderer Stelle des Förderbandes kontrolliert, ob es fehlerhafte Objekte gibt, und diese ggf. greift und entfernt. In diesem Falle sollte das Band so lange angehalten werden, bis das unerwünschte Objekt entfernt wurde.\n",
    "\n",
    "<img src=\"res/python_image_tresh_red.png\" width=\"600\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "00e460fe-d9dd-4d16-983b-21cac3779caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separiert runde, blau/rote Objekte von allen restlichen\n",
    "#Letzlich landen alle im selben Top, weil es keine runden Objekte gibt\n",
    "\n",
    "#WELCHEN TYP HABEN RUNDE OBJEKTE?\n",
    "\n",
    "offset_size = 0.05\n",
    "workspace_name = 'gazebo_1'\n",
    "# The pick pose\n",
    "pick_pose = PoseObject(\n",
    "    x=0.25, y=0., z=0.15,\n",
    "    roll=-0.0, pitch=1.57, yaw=0.0,\n",
    ")\n",
    "# The Place pose\n",
    "place_pose = PoseObject(\n",
    "    x=0.0, y=-0.25, z=0.1,\n",
    "    roll=0.0, pitch=1.57, yaw=-1.57)\n",
    "\n",
    "delete_pose= PoseObject(\n",
    "    x=0.225, y=0.32, z=0.08,\n",
    "    roll=-0.0, pitch=1.4, yaw=0.8)\n",
    "\n",
    "robot.arm.move_pose(observation_pose)\n",
    "\n",
    "#robot.arm.set_learning_mode(True)\n",
    "obj_found, shape_ret, color_ret = robot.vision.vision_pick(workspace_name)\n",
    "catch_count = 0\n",
    "delete_count= 0\n",
    "while obj_found:\n",
    "    next_place_pose = place_pose.copy_with_offsets(x_offset= catch_count * offset_size)\n",
    "    next_delete_pose= place_pose.copy_with_offsets(x_offset= catch_count * offset_size)\n",
    "    #Blau wird  gesondert platziert\n",
    "    if shape_ret is ObjectShape.CIRCLE and (color_ret is ObjectColor.BLUE or color_ret is ObjectColor.RED):\n",
    "        robot.pick_place.place_from_pose(delete_pose)\n",
    "        robot.tool.open_gripper()\n",
    "        robot.arm.move_pose(observation_pose)\n",
    "        delete_count += 1\n",
    "    #alle anderen Farben kommen auf place_pose\n",
    "    else:\n",
    "        robot.pick_place.place_from_pose(next_place_pose)\n",
    "        #zusätzlich hinzugefügt, weil er sonst nicht loslässt (Ursache unbekannt)\n",
    "        robot.tool.open_gripper()\n",
    "        robot.arm.move_pose(observation_pose)\n",
    "        catch_count +=1\n",
    "    obj_found, shape_ret, color_ret = robot.vision.vision_pick(workspace_name)\n",
    "    if obj_found == False:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f0ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Die Objkete einfach trennen im Vergleich zum vorherigen Code\n",
    "\n",
    "# -- Ihr Code hier --\n",
    "#definiere eine Bestellliste\n",
    "#Laufband läuft solange bis ein Objekt die Lichtschranke erreicht\n",
    "# Platziere jedes Objekt vom Lager auf das Förderband\n",
    "# Wenn ein Objekt die Lichtschranke erreicht:\n",
    "# überprüfe Objekt:\n",
    "# wenn richtig, lass Förderband laufen und wenn es noch Objekte im Lager gibt, mache weiter, sonst überwache weiter die Lichtschranke.\n",
    "# wenn falsch, sortiere das Objekt aus und [...]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4def487",
   "metadata": {},
   "source": [
    "#### Optional: Protokoll führen\n",
    "Erweitern Sie die Funktion um eine Protokollführung. Für die Bilanz des Unternehmens sollen die Objekte in einer Logdatei erfasst werden, die erfolgreich zur Versandabteilung geschickt wurden, sowie jene, die aufgrund von Produktionsfehlern aussortiert werden mussten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "97dca90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(<ObjectShape.SQUARE: 'SQUARE'>, <ObjectColor.GREEN: 'GREEN'>, True), (<ObjectShape.SQUARE: 'SQUARE'>, <ObjectColor.BLUE: 'BLUE'>, True), (<ObjectShape.SQUARE: 'SQUARE'>, <ObjectColor.RED: 'RED'>, True)]\n"
     ]
    }
   ],
   "source": [
    "# -- Ihr Code hier --\n",
    "\n",
    "protokoll = []\n",
    "\n",
    "def protokolliere(shape, color, correct):\n",
    "    protokoll.append((shape, color, correct))\n",
    "#Separiert runde, blau/rote Objekte von allen restlichen\n",
    "#Letzlich landen alle im selben Top, weil es keine runden Objekte gibt\n",
    "\n",
    "#WELCHEN TYP HABEN RUNDE OBJEKTE?\n",
    "\n",
    "offset_size = 0.05\n",
    "workspace_name = 'gazebo_1'\n",
    "# The pick pose\n",
    "pick_pose = PoseObject(\n",
    "    x=0.25, y=0., z=0.15,\n",
    "    roll=-0.0, pitch=1.57, yaw=0.0,\n",
    ")\n",
    "# The Place pose\n",
    "place_pose = PoseObject(\n",
    "    x=0.0, y=-0.25, z=0.1,\n",
    "    roll=0.0, pitch=1.57, yaw=-1.57)\n",
    "\n",
    "delete_pose= PoseObject(\n",
    "    x=0.225, y=0.32, z=0.08,\n",
    "    roll=-0.0, pitch=1.4, yaw=0.8)\n",
    "\n",
    "robot.arm.move_pose(observation_pose)\n",
    "\n",
    "#robot.arm.set_learning_mode(True)\n",
    "obj_found, shape_ret, color_ret = robot.vision.vision_pick(workspace_name)\n",
    "catch_count = 0\n",
    "delete_count= 0\n",
    "while obj_found:\n",
    "    next_place_pose = place_pose.copy_with_offsets(x_offset= catch_count * offset_size)\n",
    "    next_delete_pose= place_pose.copy_with_offsets(x_offset= catch_count * offset_size)\n",
    "    #Blau wird  gesondert platziert\n",
    "    if shape_ret is ObjectShape.CIRCLE and (color_ret is ObjectColor.BLUE or color_ret is ObjectColor.RED):\n",
    "        robot.pick_place.place_from_pose(delete_pose)\n",
    "        robot.tool.open_gripper()\n",
    "        robot.arm.move_pose(observation_pose)\n",
    "        delete_count += 1\n",
    "        protokolliere((shape_ret, color_ret, False))\n",
    "    #alle anderen Farben kommen auf place_pose\n",
    "    else:\n",
    "        robot.pick_place.place_from_pose(next_place_pose)\n",
    "        #zusätzlich hinzugefügt, weil er sonst nicht loslässt (Ursache unbekannt)\n",
    "        robot.tool.open_gripper()\n",
    "        robot.arm.move_pose(observation_pose)\n",
    "        catch_count +=1\n",
    "        protokolliere(shape_ret, color_ret, True)\n",
    "    obj_found, shape_ret, color_ret = robot.vision.vision_pick(workspace_name)\n",
    "    if obj_found == False:\n",
    "        break\n",
    "print(protokoll)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb18b9a",
   "metadata": {},
   "source": [
    "#### Optional: Den NED2 gesprächig machen\n",
    "Der NED2 kann sowohl visuelle als auch akustische Signale ausgeben. Der LED-Ring des NED2 kann in RGB-Farben eingestellt werden. Akustische Signale können in Tönen oder Sprache erfolgen.\n",
    "\n",
    "Fügen Sie folgende Signale hinzu, damit der NED2 über den gesamten Arbeitsverlauf kommunikativer ist:\n",
    "\n",
    "- LED-Ring soll die Farbe des gerade fokussierten Objekts annehmen, bis das Objekt gegriffen wurde. Dazu soll er dann den Satz sagen: \"*objektfarbe* Objekt erkannt\" (benötigt Internetverbindung des Roboters - aktuell nicht möglich)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e50835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (niryo)",
   "language": "python",
   "name": "niryo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
